// hooks/useAudioRecorder.ts
"use client";

import { useCallback, useRef, useState } from "react";

export type RecorderStatus = "idle" | "recording";
export type InputSource = "mic" | "system";

type Options = {
  source?: InputSource;
  /** timeslice ãƒŸãƒªç§’ï¼ˆä¾‹: 10ç§’ã”ã¨ãªã‚‰ 10000ï¼‰ undefined ãªã‚‰ stop ã¾ã§ä¸€æ‹¬ */
  timesliceMs?: number;
  /** ä½•ç§’åˆ†é€ã‚ŠãŸã„ã‹ */
  windowMs?: number;
  /** éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å—ã‘å–ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ */
  onData?: (blob: Blob, index: number) => void;
};

/**
 * éŒ²éŸ³ã ã‘è¡Œã†ãƒ•ãƒƒã‚¯
 * @param options
 * @returns
 */
export function useAudioRecorder(options?: Options) {
  const { source = "mic", timesliceMs, onData } = options ?? {};

  const [status, setStatus] = useState<RecorderStatus>("idle");
  const [stream, setStream] = useState<MediaStream | null>(null);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunkIndexRef = useRef(0);
  const chunksRef = useRef<BlobPart[]>([]);

  const start = useCallback(async () => {
    if (status === "recording") return;

    let s: MediaStream;
    if (source === "system") {
      // ğŸ–¥ ç”»é¢/ã‚¿ãƒ– + éŸ³å£°ï¼ˆå…ƒã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
      const displayStream = await navigator.mediaDevices.getDisplayMedia({
        audio: true,
        video: true,
      });

      // audio ãƒˆãƒ©ãƒƒã‚¯ã ã‘å–ã‚Šå‡ºã™
      const audioTracks = displayStream.getAudioTracks();
      if (audioTracks.length === 0) {
        throw new Error("No audio track in displayMedia stream");
      }

      // ğŸ§ éŸ³å£°ã ã‘ã®æ–°ã—ã„ MediaStream ã‚’ä½œã‚‹
      const audioOnlyStream = new MediaStream(audioTracks);

      // æ˜ åƒã¯ã‚‚ã†ä¸è¦ãªã‚‰æ­¢ã‚ã¦ãŠã
      displayStream.getVideoTracks().forEach((t) => t.stop());

      s = audioOnlyStream;
    } else {
      // ğŸ™ é€šå¸¸ãƒã‚¤ã‚¯
      s = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
        },
      });
    }
    setStream(s);

    const mr = new MediaRecorder(s, { mimeType: "audio/webm" });

    chunkIndexRef.current = 0;
    chunksRef.current = [];

    const maxChunks =
      typeof timesliceMs === "number" && options?.windowMs
        ? Math.ceil(options.windowMs / timesliceMs) // ä¾‹: 30sec / 10sec = 3
        : null;

    mr.ondataavailable = (event) => {
      if (event.data.size <= 0) return;

      // ã¾ãšå¸¸ã« push
      chunksRef.current.push(event.data);

      // ğŸ” ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ï¼šå¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’æ¨ã¦ã¦ã€Œç›´è¿‘ maxChunks å€‹ã ã‘ã€ã«ã™ã‚‹
      if (maxChunks && chunksRef.current.length > maxChunks) {
        const overflow = chunksRef.current.length - maxChunks;
        chunksRef.current.splice(0, overflow); // å…ˆé ­ã‹ã‚‰ overflow å€‹å‰Šã‚‹
      }

      if (typeof timesliceMs === "number" && onData) {
        // ç›´è¿‘ N ç§’ã¶ã‚“ã ã‘ã‚’ã¤ãªã’ãŸã€Œ1æœ¬ã® WebMã€
        const fullBlob = new Blob(chunksRef.current, {
          type: event.data.type || "audio/webm;codecs=opus",
        });

        const idx = chunkIndexRef.current++;
        onData(fullBlob, idx); // â† ã“ã“ã§ /api/transcribe ã«æŠ•ã’ã‚‹å´ã‚’å‘¼ã¶
      }
    };

    mr.onstop = () => {
      // ãƒã‚¤ã‚¯è§£æ”¾
      s.getTracks().forEach((t) => t.stop());
      setStream(null);
      setStatus("idle");
    };

    if (typeof timesliceMs === "number") {
      mr.start(timesliceMs);
    } else {
      mr.start(); // ä¸€æ‹¬éŒ²éŸ³ãƒ¢ãƒ¼ãƒ‰
    }

    mediaRecorderRef.current = mr;
    setStatus("recording");
  }, [onData, status, timesliceMs]);

  const stop = useCallback(() => {
    const mr = mediaRecorderRef.current;
    if (!mr) return;

    mr.stop();
    mediaRecorderRef.current = null;
  }, []);

  /** ä¸€æ‹¬éŒ²éŸ³ãƒ¢ãƒ¼ãƒ‰ã®ã¨ãæœ€çµ‚çš„ãª Blob ã‚’ã¾ã¨ã‚ã¦è¿”ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼ */
  const collectFullBlob = useCallback(() => {
    if (chunksRef.current.length === 0) return null;
    return new Blob(chunksRef.current, { type: "audio/webm" });
  }, []);

  return {
    status,
    stream,
    start,
    stop,
    collectFullBlob,
  };
}
